Scalable Data Pipeline for a Multi-Department Pharmaceutical Company

ğŸ“Œ Overview



This project designs a hybrid data pipeline (Lambda + Kappa) tailored for the pharmaceutical industry, handling 1M â†’ 10M+ records/day across multiple departments.

It ensures real-time insights, governance, compliance, and scalability while supporting both BI dashboards and advanced ML workflows.



ğŸ“– Contents



Executive Summary



Business Context \& Requirements



Architecture Overview



Design Pattern Choice (Hybrid)



Data Flow \& Processing Stages



Implementation Strategy



Data Quality \& Error Handling



Monitoring \& Governance



Data Lineage \& Audit Trails



GDPR \& Compliance



Roadmap \& Scalability Outlook



Conclusion



Appendix (Code References)



ğŸ—ï¸ Architecture



The pipeline follows a layered design:



Ingestion â†’ Apache Kafka (6 departmental topics).



Orchestration â†’ Apache Airflow DAGs (per department).



Processing â†’



Spark/Flink for real-time ERP \& IoT.



Spark SQL/ETL for batch sources (Clinical, Finance, Regulatory, Social).



Storage â†’ Staging â†’ Cleansed (Delta/Hudi) â†’ Presentation (Snowflake/BigQuery/Redshift).



Serving â†’ BI dashboards (Power BI, Tableau), ML feature store, compliance reporting.



Governance â†’ Amundsen/DataHub, OpenLineage, Great Expectations.



âš¡ Design Choice: Hybrid (Lambda + Kappa)



Kappa-style (stream-first) â†’ ERP \& IoT (real-time/near real-time).



Lambda-style (batch + stream) â†’ Clinical, Regulatory, Finance, Social (reproducibility, auditability).



Ensures balance between speed and reliability.



ğŸ”„ Data Flow (Example: ERP)



ERP system emits JSON â†’ Kafka (erp\_sales).



Kafka persists event â†’ Airflow DAG triggers consumer.



Data written to staging (raw + metadata).



Cleansing: validation, deduplication, pseudonymization.



Curated records stored in Delta tables â†’ Aggregations in presentation layer.



BI/ML systems consume optimized datasets.



ğŸ› ï¸ Tools \& Technologies



Ingestion: Apache Kafka, Debezium (CDC).



Orchestration: Apache Airflow.



Processing: Spark Structured Streaming, Flink, SQL.



Storage: S3/GCS + Delta Lake/Hudi + Snowflake/BigQuery/Redshift.



Governance \& Quality: Great Expectations, OpenLineage, Amundsen.



Monitoring: Prometheus + Grafana, Slack, PagerDuty.



Security: Vault/KMS, TLS, AES-256 encryption.



âœ… Features



Handles real-time + batch workloads.



Scales from 1M â†’ 10M+ records/day.



Data Quality Rules: validation, anomaly detection, DLQ with reprocessing.



Governance \& Lineage: end-to-end traceability.



GDPR Compliance: pseudonymization, right-to-be-forgotten, retention policies.



Phased Roadmap: scalable from POC â†’ enterprise-grade ML pipelines.



ğŸ‘¤ Author



Aman Singh

